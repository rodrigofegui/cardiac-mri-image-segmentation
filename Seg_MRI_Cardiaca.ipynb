{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seg_MRI_Cardiaca.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HC4K3T4SmEf_",
        "eKFfei90mcAJ",
        "EnCLKLG9og3I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC4K3T4SmEf_"
      },
      "source": [
        "# **Segmentação de MRI Cardíacas**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKFfei90mcAJ"
      },
      "source": [
        "# Dependências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z14WjMKNmnIS"
      },
      "source": [
        "Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfB9dnKSl-VP",
        "outputId": "a27e90eb-ed67-4d3b-af73-bb8324f31e5e"
      },
      "source": [
        "!pip install tensorflow==2.5.0\n",
        "!pip install scikit-learn==0.24.2\n",
        "!pip install focal-loss==0.0.7\n",
        "\n",
        "!pip install nibabel==3.2.1\n",
        "!pip install MedPy==0.4.0\n",
        "\n",
        "!pip install matplotlib==3.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.34.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.32.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.19.5)\n",
            "Requirement already satisfied: focal-loss==0.0.7 in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss==0.0.7) (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (3.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (3.3.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.19.5)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss==0.0.7) (1.34.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.2->focal-loss==0.0.7) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (57.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (1.32.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2->focal-loss==0.0.7) (3.5.0)\n",
            "Requirement already satisfied: nibabel==3.2.1 in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from nibabel==3.2.1) (21.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from nibabel==3.2.1) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->nibabel==3.2.1) (2.4.7)\n",
            "Requirement already satisfied: MedPy==0.4.0 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from MedPy==0.4.0) (1.19.5)\n",
            "Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from MedPy==0.4.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from MedPy==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib==3.2.2) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzHCSwCdnIHQ"
      },
      "source": [
        "Acesso à arquivos do Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQl43uvenSLS",
        "outputId": "d9b8ac76-1295-47d2-c01c-ff2604fdd7cb"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive_path = [\n",
        "    '/content/drive',\n",
        "    'My Drive/Colab Notebooks/data'\n",
        "]\n",
        "\n",
        "drive.mount(drive_path[0], force_remount=True)\n",
        "os.chdir('/'.join(drive_path))\n",
        "sys.path.append('/'.join(drive_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAnelmOnebU"
      },
      "source": [
        "Visualização da GPU disponibilizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TmRxKiCnnCr",
        "outputId": "b022b9d6-4b6a-47a6-fc7f-a65215c39928"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HRMYmjznryn"
      },
      "source": [
        "Visualizando a quantidade de GPUs detectadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNVZRezpnwlV",
        "outputId": "2d56229d-1f48-4996-ded2-d0a2f4ac736c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f'GPUs available: {len(gpus)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPUs available: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBDBgymJoECi"
      },
      "source": [
        "Configuração de uso gradual e necessário da memória da GPU utilizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcgIaH9XoE5r"
      },
      "source": [
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(f'{len(gpus)} physical GPUs vs. {len(logical_gpus)} logical GPUs')\n",
        "        \n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnCLKLG9og3I"
      },
      "source": [
        "# Constantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNe0BZkHokLU"
      },
      "source": [
        "IMG_SZ = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRgMFMDo1UC"
      },
      "source": [
        "RANDOM_SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FiWg76aptvV"
      },
      "source": [
        "Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ4EW5HCRpe"
      },
      "source": [
        "PATCH_STEP = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwnnmbtnn3_S"
      },
      "source": [
        "DT_BUFFER_SZ = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKj41ofVL5x"
      },
      "source": [
        "VALIDATION_PROPORTION = .2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UDTg4lo7GY"
      },
      "source": [
        "XVALIDATION_PROPORTION = .65"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZ1vFnBo86-"
      },
      "source": [
        "# represent voxels located in the:\n",
        "# 0: background\n",
        "# 1: RV cavity\n",
        "# 2: myocardium\n",
        "# 3: LV cavity \n",
        "CLASSES_CNT = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FypEp4tTqFOz"
      },
      "source": [
        "TRAINING_FILENAME = 'training.zip'\n",
        "TRAINING_DIR = 'ACDC/train'\n",
        "\n",
        "TRAINING_FILENAME = os.path.join(*drive_path, TRAINING_FILENAME)\n",
        "TRAINING_DIR = os.path.join(*drive_path, TRAINING_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tTzWDkqmFU"
      },
      "source": [
        "Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7z_CNNqoiu"
      },
      "source": [
        "MODEL_FILENAME = 'ulas_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW8kwOYwqsS2"
      },
      "source": [
        "MODEL_DIR = os.path.join(*drive_path, MODEL_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_VUxpZxt5IE"
      },
      "source": [
        "Métricas e Funções de perda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHWuvgnRt4Vx"
      },
      "source": [
        "TVERSKY_FN_WEIGHT = .7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmOYOCnt_Gg"
      },
      "source": [
        "TVERSKY_FOCAL = .5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0NCWDkVuDMP"
      },
      "source": [
        "FOCAL_LOSS_GAMA = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abjYtJ7zp2Bj"
      },
      "source": [
        "Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPy_ujYcpDiT"
      },
      "source": [
        "INIT_LEARNING_RATE = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DExT9SdYpNCm"
      },
      "source": [
        "MIN_LEARNING_RATE = 1e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAAyIdURprUw"
      },
      "source": [
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7LBrU0Ptpw9"
      },
      "source": [
        "MAIN_METRIC = 'dice_index_m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIERQ-Kqtt3Z"
      },
      "source": [
        "if MODEL_FILENAME == 'unet_plusplus_model':\n",
        "    EVAL_METRIC = f're_lu_{MAIN_METRIC}'\n",
        "else:\n",
        "    EVAL_METRIC = MAIN_METRIC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_lgkcSdrq25"
      },
      "source": [
        "Milestones do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp0RTecqsvC5"
      },
      "source": [
        "EARLY_STOP_PATIENCE = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asIAvRtHs5Rh"
      },
      "source": [
        "STEPS_FILENAME = os.path.join(\n",
        "    MODEL_DIR,\n",
        "    '{}_improvement_{}.h5'.format(\n",
        "        MODEL_FILENAME,\n",
        "        '{}_{}'.format(\n",
        "            '{epoch:02d}',\n",
        "            ''.join(['{', f'val_{EVAL_METRIC}:.2f', '}'])\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8OTM6KNs-vk"
      },
      "source": [
        "CSV_LOG_FILENAME = os.path.join(MODEL_DIR, f'{MODEL_FILENAME}_log.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkYM5QDBtPul"
      },
      "source": [
        "Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAkIzuNPtOzR"
      },
      "source": [
        "PLOT_FILENAME = f'{MODEL_FILENAME}_structure.png'\n",
        "PLOT_FILENAME = os.path.join(MODEL_DIR, PLOT_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8k9BznKtWcu"
      },
      "source": [
        "PLOT_WIDTH = 720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRA4xxMDuLRG"
      },
      "source": [
        "# Aquisição das imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLKawJgfvLcm"
      },
      "source": [
        "Imports desta seção"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DctGw4bvN7t"
      },
      "source": [
        "import os\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.image import extract_patches\n",
        "from tensorflow.keras.utils import normalize, to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fosux3OuQkB"
      },
      "source": [
        "\"\"\"\n",
        "author: Clément Zotti (clement.zotti@usherbrooke.ca)\n",
        "date: April 2017\n",
        "\n",
        "DESCRIPTION :\n",
        "The script provide helpers functions to handle nifti image format:\n",
        "    - load_nii()\n",
        "    - save_nii()\n",
        "\n",
        "to generate metrics for two images:\n",
        "    - metrics()\n",
        "\n",
        "And it is callable from the command line (see below).\n",
        "Each function provided in this script has comments to understand\n",
        "how they works.\n",
        "\n",
        "HOW-TO:\n",
        "\n",
        "This script was tested for python 3.4.\n",
        "\n",
        "First, you need to install the required packages with\n",
        "    pip install -r requirements.txt\n",
        "\n",
        "After the installation, you have two ways of running this script:\n",
        "    1) python metrics.py ground_truth/patient001_ED.nii.gz prediction/patient001_ED.nii.gz\n",
        "    2) python metrics.py ground_truth/ prediction/\n",
        "\n",
        "The first option will print in the console the dice and volume of each class for the given image.\n",
        "The second option wiil ouput a csv file where each images will have the dice and volume of each class.\n",
        "\n",
        "\n",
        "Based on: http://acdc.creatis.insa-lyon.fr\n",
        "\"\"\"\n",
        "#\n",
        "# Utils function to load and save nifti files with the nibabel package\n",
        "#\n",
        "def load_nii(img_path):\n",
        "    \"\"\"\n",
        "    Function to load a 'nii' or 'nii.gz' file, The function returns\n",
        "    everyting needed to save another 'nii' or 'nii.gz'\n",
        "    in the same dimensional space, i.e. the affine matrix and the header\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    img_path: string\n",
        "    String with the path of the 'nii' or 'nii.gz' image file name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Three element, the first is a numpy array of the image values,\n",
        "    the second is the affine transformation of the image, and the\n",
        "    last one is the header of the image.\n",
        "    \"\"\"\n",
        "    nimg = nib.load(img_path)\n",
        "    return nimg.get_fdata().astype('float32'), nimg.affine, nimg.header\n",
        "\n",
        "def save_nii(img_path, data, affine, header):\n",
        "    \"\"\"\n",
        "    Function to save a 'nii' or 'nii.gz' file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    img_path: string\n",
        "    Path to save the image should be ending with '.nii' or '.nii.gz'.\n",
        "\n",
        "    data: np.array\n",
        "    Numpy array of the image data.\n",
        "\n",
        "    affine: list of list or np.array\n",
        "    The affine transformation to save with the image.\n",
        "\n",
        "    header: nib.Nifti1Header\n",
        "    The header that define everything about the data\n",
        "    (pleasecheck nibabel documentation).\n",
        "    \"\"\"\n",
        "    nimg = nib.Nifti1Image(data, affine=affine, header=header)\n",
        "    nimg.to_filename(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rDOEKiCvsVz"
      },
      "source": [
        "def file_paths(base_dir):\n",
        "    c_paths = {\n",
        "        'config': [],\n",
        "        'ground_truth': [],\n",
        "        'images': [],\n",
        "        '4d': []\n",
        "    }\n",
        "\n",
        "    for c_dir, next_dirs, file_names in os.walk(base_dir):\n",
        "        # skip root\n",
        "        if next_dirs:\n",
        "            continue\n",
        "\n",
        "        c_patient = os.path.basename(c_dir)\n",
        "\n",
        "        for f_n in file_names:\n",
        "            f_n = os.path.join(c_dir, f_n)\n",
        "\n",
        "            if f_n.endswith('.cfg'): file_type = 'config'\n",
        "            elif '_gt' in f_n: file_type = 'ground_truth'\n",
        "            elif '_4d' in f_n: file_type = '4d'\n",
        "            else: file_type = 'images'\n",
        "\n",
        "            c_paths[file_type].append(f_n)\n",
        "\n",
        "    for file_type in c_paths.keys():\n",
        "        c_paths[file_type].sort()\n",
        "\n",
        "    return c_paths\n",
        "\n",
        "def load_data(file_paths, is_training=True):\n",
        "    images_filename = os.path.join(*drive_path, f'{\"training\" if is_training else \"test\"}_data.npy')\n",
        "    gt_filename = os.path.join(*drive_path, f'{\"training\" if is_training else \"test\"}_gt_data.npy')\n",
        "\n",
        "    try:\n",
        "        imgs = np.load(images_filename, allow_pickle=True)\n",
        "        imgs_gt = np.load(gt_filename, allow_pickle=True)\n",
        "\n",
        "        if np.any(imgs) and np.any(imgs_gt):\n",
        "            print('carregado dos arquivos')\n",
        "\n",
        "            return tf.convert_to_tensor(imgs), tf.convert_to_tensor(imgs_gt)\n",
        "    except:\n",
        "        print('carregando arquivos')\n",
        "        imgs, imgs_gt =  _load_data(file_paths)\n",
        "\n",
        "        # np.save(images_filename, imgs)\n",
        "        # np.save(gt_filename, imgs_gt)\n",
        "\n",
        "        return imgs, imgs_gt\n",
        "\n",
        "def _load_data(file_paths):\n",
        "    images, ground_truth = [], []\n",
        "    cnt = 1\n",
        "\n",
        "    for c_img, c_ground_truth in zip(file_paths['images'], file_paths['ground_truth']):\n",
        "        print(f'processing {cnt}...')\n",
        "\n",
        "        # we load 3D training image\n",
        "        training_image, _, _ = load_nii(c_img)\n",
        "        # we load 3D training mask (shape=(512,512,129))\n",
        "        train_ground_truth, _, _ = load_nii(c_ground_truth)\n",
        "\n",
        "        for k in range(min(train_ground_truth.shape[-1], training_image.shape[-1])):\n",
        "            #axial cuts are made along the z axis with undersampling\n",
        "            gt_2d = np.array(train_ground_truth[::, ::, k])\n",
        "\n",
        "            # invalid ground truth\n",
        "            if len(np.unique(gt_2d)) == 1:\n",
        "                continue\n",
        "\n",
        "            image_patches = _extract_patches(np.array(training_image[::, ::, k]))\n",
        "            gt_patches = _extract_patches(gt_2d)\n",
        "\n",
        "            print('uniques gt_2d:', unique(gt_2d).shape)\n",
        "            print('uniques gt_patches:', unique(gt_patches).shape)\n",
        "            print()\n",
        "\n",
        "            if (tf.size(images) == 0).numpy():\n",
        "                images = image_patches\n",
        "            else:\n",
        "                images = tf.concat((images, image_patches), axis=0)\n",
        "\n",
        "            if (tf.size(ground_truth) == 0).numpy():\n",
        "                ground_truth = gt_patches\n",
        "            else:\n",
        "                ground_truth = tf.concat((ground_truth, gt_patches), axis=0)\n",
        "\n",
        "        if cnt >= 10:\n",
        "            break\n",
        "\n",
        "        cnt += 1\n",
        "\n",
        "    images = normalize(images)\n",
        "    tf.cast(ground_truth, tf.uint8)\n",
        "\n",
        "    return images, ground_truth\n",
        "\n",
        "def _extract_patches(image_2d):\n",
        "    image_2d = tf.expand_dims(tf.expand_dims(image_2d, axis=-1), axis=0)\n",
        "\n",
        "    if image_2d.shape[1] < IMG_SZ or image_2d.shape[2] < IMG_SZ:\n",
        "        return tf.image.resize(\n",
        "            image_2d, (IMG_SZ, IMG_SZ), method='nearests'\n",
        "        )\n",
        "\n",
        "    image_patches = extract_patches(\n",
        "        image_2d,\n",
        "        sizes=[1, IMG_SZ, IMG_SZ, 1],\n",
        "        strides=[1, PATCH_STEP, PATCH_STEP, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "\n",
        "    return tf.reshape(\n",
        "        image_patches,\n",
        "        [image_patches.shape[1] * image_patches.shape[2], IMG_SZ, IMG_SZ, 1]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBnxc1TaUIsT"
      },
      "source": [
        "# Carregando dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJBLyaJoUSef"
      },
      "source": [
        "Imports da seção"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCDybhaqURto"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import unique\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAtsCK5AnFJF"
      },
      "source": [
        "def to_dataset(data, labels):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "    dataset = dataset.shuffle(\n",
        "        buffer_size=DT_BUFFER_SZ,\n",
        "        seed=RANDOM_SEED,\n",
        "        reshuffle_each_iteration=True\n",
        "    )\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.prefetch(2)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgYXN9Mz5Lpl"
      },
      "source": [
        "training_paths = file_paths(TRAINING_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywop8DyCka3Y",
        "outputId": "7452d88b-9aed-4d08-8ed9-a7d6e928da37"
      },
      "source": [
        "print(len(training_paths['config']))\n",
        "print(len(training_paths['ground_truth']))\n",
        "print(len(training_paths['images']))\n",
        "print(len(training_paths['4d']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "200\n",
            "99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRbfZ2Mw6Jmh"
      },
      "source": [
        "raw_train_data, raw_train_labels = load_data(training_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ4UDaVRGYrw",
        "outputId": "8a31a846-76cb-40f9-8932-f60e6aee72ff"
      },
      "source": [
        "print(f'raw_train_data: {raw_train_data.shape}\\n\\t{raw_train_data.dtype}')\n",
        "print(f'raw_train_labels: {raw_train_labels.shape}\\n\\t{raw_train_labels.dtype}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw_train_data: (1480, 200, 200, 1)\n",
            "\t<dtype: 'float32'>\n",
            "raw_train_labels: (1480, 200, 200, 1)\n",
            "\t<dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQASGF_bFW8",
        "outputId": "7596cf64-040a-45fa-90f7-2094cad9e681"
      },
      "source": [
        "classes_weight = compute_class_weight(\n",
        "    'balanced',\n",
        "    unique(raw_train_labels),\n",
        "    tf.reshape(raw_train_labels, tf.size(raw_train_labels)).numpy()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0.0000000e+00 1.3022672e-03 1.3023914e-03 ... 3.0000517e+00 3.0000520e+00\n",
            " 3.0000541e+00], y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTr9988Rb7Du",
        "outputId": "886d79c3-7d82-46d5-cd0d-23f59e5626c3"
      },
      "source": [
        "print(f'classes_weight: {len(classes_weight)}\\n{classes_weight}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classes_weight: 7298\n",
            "[1.46106375e-04 2.61934777e+04 2.61934777e+04 ... 1.66837437e+02\n",
            " 7.48385076e+02 3.15584068e+02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQeNPTW7ubNb",
        "outputId": "0d76cc01-ec32-49ff-8eff-a00b9e37b267"
      },
      "source": [
        "unique(raw_train_labels).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC3AagLlbY9u"
      },
      "source": [
        "raw_train_labels = to_categorical(raw_train_labels, CLASSES_CNT, dtype='uint8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h82EnnK7Uxls"
      },
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    raw_train_data, raw_train_labels,\n",
        "    test_size=VALIDATION_PROPORTION,\n",
        "    random_state=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOV5wKxOV71r"
      },
      "source": [
        "print(f'test_data: {test_data.shape}')\n",
        "print(f'test_labels: {test_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TILS5we-WH6M"
      },
      "source": [
        "train_data, xval_data, train_labels, xval_labels = train_test_split(\n",
        "    train_data, train_labels,\n",
        "    test_size=XVALIDATION_PROPORTION,\n",
        "    random_state=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoRxciPWSAi"
      },
      "source": [
        "print(f'train_data: {train_data.shape}')\n",
        "print(f'xval_data: {xval_data.shape}')\n",
        "print(f'train_labels: {train_labels.shape}')\n",
        "print(f'xval_labels: {xval_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TDvEa1goyJb"
      },
      "source": [
        "train_dataset = to_dataset(train_data, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrdS_4yAo4B8"
      },
      "source": [
        "xval_dataset = to_dataset(xval_data, xval_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkfqjoc3o4b-"
      },
      "source": [
        "test_dataset = to_dataset(test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dOZ5954W5Df"
      },
      "source": [
        "Pré-visualização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1DaxUYnX7vu"
      },
      "source": [
        "rnd_idx = randint(0, len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE2UYdjYwCB"
      },
      "source": [
        "fig, axes = plt.subplots(1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rx_lhqAYyoN"
      },
      "source": [
        "axes[0].imshow(train_data[rdn_idx], cmap='hot')\n",
        "axes[1].imshow(train_labels[rdn_idx], cmap='hot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coNuYULHZSWj"
      },
      "source": [
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}